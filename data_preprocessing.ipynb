{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3JZ3hRJ9f1O",
        "outputId": "3380cf91-a86a-4756-b0be-e6a9c1f89c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_Preprocessing():\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.text_list = []\n",
        "    \n",
        "    def tokenize_text(self): # convert sentences into words\n",
        "      for txt in self.text:\n",
        "        text = word_tokenize(txt)\n",
        "        self.text_list.append(text)\n",
        "      self.text = self.text_list\n",
        "      self.text_list = []\n",
        "      return\n",
        "\n",
        "    def remove_stopwords(self): # remove english stop words from tokenized text\n",
        "      for txt in self.text:\n",
        "        text = [word for word in txt if not word in stopwords.words()]\n",
        "        self.text_list.append(text)\n",
        "      self.text = self.text_list\n",
        "      self.text_list = []\n",
        "      return \n",
        "\n",
        "    def lemmatize_text(self): # apply lemmatization on the data\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      for txt in self.text:\n",
        "        text = ' '.join([lemmatizer.lemmatize(w) for w in txt])\n",
        "        text = text.split()\n",
        "        self.text_list.append(text)\n",
        "      self.text = self.text_list\n",
        "      self.text_list = []\n",
        "      return self.text\n",
        "\n",
        "# return he words to their initial format, \n",
        "#but I will no use it as it can affect on the produced topics. \n",
        "#As in somecases some different words will have the same shape after stemming\n",
        "    def stemming_text(self): \n",
        "      stem_list = []\n",
        "      ps = PorterStemmer()\n",
        "      for txt in self.text:\n",
        "        for w in txt:\n",
        "          stem_list.append(ps.stem(w))\n",
        "        text = ' '.join(stem_list)\n",
        "        text = text.split()\n",
        "        self.text_list.append(text)\n",
        "        stem_list = []\n",
        "      self.text = self.text_list\n",
        "      self.text_list = []\n",
        "      return self.text\n",
        "\n",
        "    def create_data_dictionary(self):\n",
        "      dictionary = gensim.corpora.Dictionary(self.text)\n",
        "      return dictionary\n",
        "\n",
        "    def create_corpus_dataset(self,dictionary):\n",
        "      bow_corpus = [dictionary.doc2bow(doc) for doc in self.text]\n",
        "      return bow_corpus\n",
        "\n",
        "    def preprocessing_dataset(self):\n",
        "      self.tokenize_text()\n",
        "      self.remove_stopwords()\n",
        "      lemma_text = self.lemmatize_text()\n",
        "      # stemm_text = self.stemming_text()\n",
        "      dictionary = self.create_data_dictionary()\n",
        "      bow_corpus = self.create_corpus_dataset(dictionary)\n",
        "      return lemma_text, dictionary, bow_corpus"
      ],
      "metadata": {
        "id": "qkzOtRN49lxK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_xfJhC7__jRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}